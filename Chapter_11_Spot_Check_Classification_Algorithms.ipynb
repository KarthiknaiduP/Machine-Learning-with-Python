{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "You cannot know which algorithm will work best on your dataset beforehand. You must use trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking."
      ],
      "metadata": {
        "id": "7UCM3w1SzFXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms Overview**\n",
        "\n",
        "Starting with two linear machine learning algorithms\n",
        "\n",
        "* Logistic Regression.\n",
        "* Linear Discriminant Analysis.\n",
        "\n",
        "Then looking at four nonlinear machine learning algorithms:\n",
        "\n",
        "* k-Nearest Neighbors.\n",
        "* Naive Bayes.\n",
        "* Classification and Regression Trees. Ùè∞Ä Support Vector Machines."
      ],
      "metadata": {
        "id": "aiLUxE2yzKt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Logistic Regression**\n",
        "\n"
      ],
      "metadata": {
        "id": "sS3h4v3X0ZLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression assumes a Guassian distribution for the input variables and can mosel binary classification problems.\n",
        "\n",
        "you can cnstruct a logistic regression model using the LogisticRegression class"
      ],
      "metadata": {
        "id": "EmBjGtZ30n0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "filename = '/content/diabetes 2.csv'\n",
        "dataframe = read_csv(filename)\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "array = dataframe.values\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10,shuffle=True, random_state=7)\n",
        "model = LogisticRegression()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yixo8zXo1Sd8",
        "outputId": "073822c7-417f-4bcd-c868-0f58a5bf4d2e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7760423786739576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Linear Discriminant Analysis**"
      ],
      "metadata": {
        "id": "1nZGSqu-2Hd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables. You can construct an LDA model using the LinearDiscriminantAnalysis class2."
      ],
      "metadata": {
        "id": "n9wm_Wn02exG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import KFold\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10,shuffle=True, random_state=7)\n",
        "model = LinearDiscriminantAnalysis()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mChvzfN2g5e",
        "outputId": "1bcc9cae-ce8c-4bbb-9a84-0f6f435a4d5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7669685577580315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. k-Nearest Neighbors**"
      ],
      "metadata": {
        "id": "fCWXvrtk24mW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class."
      ],
      "metadata": {
        "id": "JViYhAtx3AY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW-UgV1t3KgN",
        "outputId": "afb1c257-b1a9-427d-ff5e-1bdb515ca897"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7109876965140123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Naive Bayes**\n"
      ],
      "metadata": {
        "id": "M1hUPFiT3OKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. You can construct a Naive Bayes model using the GaussianNB class4."
      ],
      "metadata": {
        "id": "e6RHaCsv3Tea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y8Au3Xc8TM2",
        "outputId": "f1ece1bc-c36c-4697-d005-e254e21e3e6c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7591421736158578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Classification and Regression Trees**"
      ],
      "metadata": {
        "id": "PDyzrlHi-BQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index). You can construct a CART model using the DecisionTreeClassifier class5."
      ],
      "metadata": {
        "id": "JMz8VFCY-Qrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnJIr7zt_GGq",
        "outputId": "9eddd89a-3d37-4c33-fa91-e655176370e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6876452494873547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Support Vector Machines**"
      ],
      "metadata": {
        "id": "fcVJ1N9x_Li6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter.\n",
        "\n",
        "You can construct an SVM model using the\n",
        "SVC class6."
      ],
      "metadata": {
        "id": "gNbhTbxL_QOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awpmyIUEB1y2",
        "outputId": "23737cf8-724e-4386-b509-928647d3a8ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6915242652084757\n"
          ]
        }
      ]
    }
  ]
}